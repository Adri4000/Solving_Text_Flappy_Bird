{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC and Sarsa($\\lambda$) Controls on Text Flappy Bird (TFB)\n",
    "### Author : Adrien Fu\n",
    "### Date : 28/03/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import gymnasium as gym\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import text_flappy_bird_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate environment\n",
    "env = gym.make('TextFlappyBird-screen-v0', height = 15, width = 20, pipe_gap = 4)\n",
    "# 2 Choices : 'TextFlappyBird-v0', 'TextFlappyBird-screen-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly test the TFB environment with random action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "\n",
    "# iterate\n",
    "while True:\n",
    "\n",
    "    # Select next action\n",
    "    action = env.action_space.sample()  # for an agent, action = agent.policy(observation)\n",
    "\n",
    "    # Appy action and return new observation of the environment\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    \n",
    "    # Render the game\n",
    "    os.system(\"clear\")\n",
    "    sys.stdout.write(env.render())\n",
    "    time.sleep(0.2) # FPS\n",
    "    \n",
    "    # Clear the output to make the frames dynamic\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # If player is dead break\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For `\"TextFlappyBird-screen-v0\"`, at time $t$, each observation $O_t$ is the complete screen render of the game (an array of size width x height). This array is filled of $0$, except for the position of the bird (number $1$) and the obstacles (located by the number $2$). When the bird hits an obstacle, it dies and the position has value $3$.\n",
    "\n",
    "- At each time $t$, the bird has 2 choices of action : idle (__action__ = $0$) or flap (__action__ = $1$).\n",
    "\n",
    "- Flapping allows the position of the bird to be 1 block higher, and idling makes the bird go lower for the next time step $t+1$. To be more precise, if the bird idles 3 times in a row, the first time the bird falls by 1 block, the second time by 2 blocks, and the third time by 3 blocks. \n",
    "\n",
    "- To define the __state__ from the observation, we define it as the vector $(x,y)$ that goes from the bird to the center of the closest upcoming pipe gap. That way, the trained agents can be used for `\"TextFlappyBird-v0\"`, as the observation in `\"TextFlappyBird-v0\"` is direclty the vector $(x,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 :  Constant $\\alpha$-MC control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first write the function `mc_control` that is an implementation of constant-$\\alpha$ MC control.\n",
    "\n",
    "This algorithm has six arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate. It must be a value between $0$ and $1$, inclusive (default value: $1$).\n",
    "- `get_score_every_it`: Every `get_score_every_it` episodes, we test the trained policy $100$ times, and get a mean score that will be used below to plot the evolution of the score when training.\n",
    "- `max_reward_value`: When we test the trained policy, `max_reward_value` is the maximum value of the reward before going to the next iteration. This avoids too long testing time.\n",
    "\n",
    "The algorithm returns:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "- `policy`: This is a dictionary where `policy[s]` returns the action that the agent chooses after observing state `s`.\n",
    "- `L_mean_score`: A list of mean scores, where each one is from testing the policy $100$ times at a fixed episode.\n",
    "- `L_std_score`: The list of standard deviations corresponding to `L_mean_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    \"\"\" gets the state from the observation \"\"\"\n",
    "    width, height = obs.shape\n",
    "    \n",
    "    # in the case the player is dead, return the state \"dead\"\n",
    "    if 1 in obs:\n",
    "        x_bird, y_bird = np.argwhere(obs == 1)[0]\n",
    "    else:\n",
    "        return \"dead\"\n",
    "    \n",
    "    # if the player is still alive, state = vector from bird to the center of the closest upcoming pipe gap\n",
    "    for i in range(x_bird, width):\n",
    "        if obs[i,0] == 2 or obs[i,-1] == 2:\n",
    "            x_obstacle = i\n",
    "            break\n",
    "    for i in range(height):\n",
    "        if obs[x_obstacle, i] == 0:\n",
    "            for j in range(i,height):\n",
    "                if obs[x_obstacle, j] == 2:\n",
    "                    y_obstacle = (i+j)//2\n",
    "                    break\n",
    "    state = (x_obstacle-x_bird, y_obstacle-y_bird)\n",
    "    return state\n",
    "\n",
    "\n",
    "def get_probs(Q_s, epsilon, nA):\n",
    "    \"\"\" obtains the action probabilities corresponding to epsilon-greedy policy \"\"\"\n",
    "    policy_s = np.ones(nA) * epsilon /nA\n",
    "    best_a = np.argmax(Q_s)\n",
    "    \n",
    "    policy_s[best_a] = 1 - epsilon + epsilon / nA\n",
    "    return policy_s\n",
    "\n",
    "\n",
    "def generate_episode_from_Q(env, Q, epsilon, nA, max_reward):\n",
    "    \"\"\" generates an episode from following the epsilon-greedy policy \"\"\"\n",
    "    episode = []\n",
    "    obs, _ = env.reset()\n",
    "    state = get_state(obs)\n",
    "    \n",
    "    for _ in range(max_reward):\n",
    "        action = np.random.choice(np.arange(nA), p=get_probs(Q[state], epsilon, nA))\\\n",
    "                                                    if state in Q else env.action_space.sample()\n",
    "                \n",
    "        next_obs, reward, done, _, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        obs = next_obs\n",
    "        state = get_state(obs)\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `one_play` below runs a game of TFB to test a policy. It has four arguments:\n",
    "\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `policy`: This is the tested policy.\n",
    "- `graphic`: If `graphic = False`, the game runs with no FPS and returns the final score. If `graphic = True`, the game runs normally, and return nothing.\n",
    "- `max_step`: The maximum number of steps before ending the game. This avoids too long testing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to test a policy in one game, returns a score if \n",
    "def one_play(env, policy, graphic = False, max_reward = 4000, FPS=5, return_type=\"score\"):\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    sum_reward = 0\n",
    "    for _ in range(max_reward):\n",
    "        # Select next action\n",
    "        state = get_state(obs)\n",
    "        action = policy[state] if state in policy else env.action_space.sample()\n",
    "\n",
    "        # Appy action and return new observation of the environment\n",
    "        obs, r, done, _, info = env.step(action)\n",
    "        sum_reward += r\n",
    "\n",
    "        if graphic:\n",
    "            # Clear the output to make things dynamic\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            # Render the game\n",
    "            os.system(\"clear\")\n",
    "            sys.stdout.write(env.render())\n",
    "            time.sleep(1/FPS) # FPS\n",
    "            \n",
    "        # If player is dead break\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    \n",
    "    if graphic == False:\n",
    "        if return_type == \"score\":\n",
    "            return info[\"score\"]\n",
    "        else:\n",
    "            return sum_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_update_Q(episode, Q, alpha, gamma):\n",
    "    \"\"\" updates the action-value function estimate using the most recent episode \"\"\"\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    # prepare for discounting\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    \n",
    "    for i, state in enumerate(states):\n",
    "        old_Q = Q[state][actions[i]]\n",
    "        Q[state][actions[i]] = old_Q + alpha * (sum(rewards[i:]*discounts[:-(1+i)])- old_Q)\n",
    "    return Q\n",
    "\n",
    "\n",
    "def mc_control(env, num_episodes, alpha, gamma=1.0, eps_start=1.0, eps_decay=.99999,\n",
    "               eps_min=0.05, get_score_every_it = 1000, max_reward = 4000):\n",
    "    nA = env.action_space.n\n",
    "    epsilon = eps_start\n",
    " \n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    \n",
    "    # retrieve the score every 1000 episodes, with the current policy\n",
    "    L_mean_score = []\n",
    "    L_std_score = []\n",
    "    \n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        if i_episode % get_score_every_it == 0:\n",
    "            # policy test to get the sum of reward (limited to max_step  = 2000)\n",
    "            policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "            L_score = []\n",
    "            for _ in range(100):\n",
    "                L_score.append(one_play(env, policy, graphic = False, max_reward = max_reward))\n",
    "            \n",
    "            L_mean_score.append(np.mean(L_score))\n",
    "            L_std_score.append(np.std(L_score))\n",
    "            \n",
    "        \n",
    "        # set the value of epsilon\n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "        # generate an episode by following epsilon-greedy policy\n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA, max_reward)\n",
    "        # update the action-value function estimate using the episode\n",
    "        Q = MC_update_Q(episode, Q, alpha, gamma)\n",
    "        \n",
    "\n",
    "    # determine the policy corresponding to the final action-value function estimate\n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "    L_mean_score = np.array(L_mean_score)\n",
    "    L_std_score = np.array(L_std_score)\n",
    "    return Q, policy, L_mean_score, L_std_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs 3-9 mins to run this cell\n",
    "MC_Q, MC_policy, MC_L_mean_score, MC_L_std_score = mc_control(env, num_episodes=20000, alpha=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_score = []\n",
    "for i in range(100):\n",
    "    L_score.append(one_play(env, MC_policy, graphic = False))\n",
    "print(f\"Mean score with MC: {np.mean(L_score)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimated optimal state-value functions and policy of MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `plot_values` below shows the estimated optimal state-value functions and policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_values(Q):\n",
    "\n",
    "    x_coords, y_coords = zip(*Q.keys())  # Unpack the keys into x and y coordinates\n",
    "\n",
    "    # Create a grid for plotting\n",
    "    # Here, we assume the x and y coordinates form a grid from min to max values\n",
    "    x_min, x_max = min(x_coords), max(x_coords)\n",
    "    y_min, y_max = min(y_coords), max(y_coords)\n",
    "\n",
    "    # Create the grid\n",
    "    X, _ = np.meshgrid(np.arange(x_min, x_max + 1), np.arange(y_min, y_max + 1))\n",
    "\n",
    "    # Create a value matrix based on the dictionary\n",
    "    V0, V1, P = np.zeros_like(X), np.zeros_like(X), np.zeros_like(X)\n",
    "    for (x, y), value in Q.items():\n",
    "        x_idx = x - x_min  # Normalize the index\n",
    "        y_idx = y - y_min  # Normalize the index\n",
    "        V0[y_idx, x_idx] = value[0]\n",
    "        V1[y_idx, x_idx] = value[1]\n",
    "        P[y_idx, x_idx] = np.argmax(value)\n",
    "\n",
    "    # Plot the grid using imshow (heatmap)\n",
    "\n",
    "    plt.figure(figsize=(16, 7))\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(V0, cmap='viridis', extent=[x_min, x_max, y_min, y_max], origin='lower', interpolation='nearest')\n",
    "    plt.title(f\"State-value function with action = 0 (Idle)\")\n",
    "    plt.xlabel(\"x coordinate to the pipe gap center\")\n",
    "    plt.ylabel(\"y coordinate to the pipe gap center\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(V1, cmap='viridis', extent=[x_min, x_max, y_min, y_max], origin='lower', interpolation='nearest')\n",
    "    plt.title(f\"State-value function with action = 1 (Flap)\")\n",
    "    plt.xlabel(\"x coordinate to the pipe gap center\")\n",
    "    plt.ylabel(\"y coordinate to the pipe gap center\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(P, cmap='viridis', extent=[x_min, x_max, y_min, y_max], origin='lower', interpolation='nearest')\n",
    "    plt.title(f\"Deterministic policy (1 = Flap, 0 = Idle)\")\n",
    "    plt.xlabel(\"x coordinate to the pipe gap center\")\n",
    "    plt.ylabel(\"y coordinate to the pipe gap center\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_values(MC_Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Remark</u>: We have defined the policy as $\\pi(s) = \\text{argmax}_{a} Q(s,a)$, so each square of the heatmap of the policy is just the action that maximizes the same square in the state-value heatmaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To play TFB with `MC_policy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_play(env, MC_policy, graphic = True, max_step = 4000, FPS=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 : Sarsa($\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function `lambda_sarsa` that is an implementation of Sarsa($\\lambda$) control.\n",
    "\n",
    "This algorithm has six arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate. It must be a value between $0$ and $1$, inclusive (default value: $1$).\n",
    "- `lmbda`: This is the lambda parameter of the algorithm (default value: $0.9$).\n",
    "- `get_score_every_it`: Every `get_score_every_it` episodes, we test the trained policy $100$ times, and get a mean score that will be used below to plot the evolution of the score when training.\n",
    "- `max_reward_value`: When we test the trained policy, `max_reward_value` is the maximum value of the reward before going to the next iteration. This avoids too long testing time.\n",
    "\n",
    "The algorithm returns:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "- `policy`: This is a dictionary where `policy[s]` returns the action that the agent chooses after observing state `s`.\n",
    "- `L_mean_score`: A list of mean scores, where each one is from testing the policy $100$ times at a fixed episode.\n",
    "- `L_std_score`: The list of standard deviations corresponding to `L_mean_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_sarsa(env, num_episodes, alpha, gamma=1.0, lmbda=0.9, eps_start=1.0, eps_decay=.99999,\n",
    "               eps_min=0.05, get_score_every_it = 1000, max_reward = 4000):\n",
    "    \n",
    "    nA = env.action_space.n\n",
    "    epsilon = eps_start\n",
    " \n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    \n",
    "    # retrieve the score every 1000 episodes, with the current policy\n",
    "    L_mean_score = []\n",
    "    L_std_score = []\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        if i_episode % get_score_every_it == 0:\n",
    "            # policy test to get the sum of reward (limited to max_step  = 2000)\n",
    "            policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "            L_score = []\n",
    "            for _ in range(100):\n",
    "                L_score.append(one_play(env, policy, graphic=False, max_reward=max_reward))\n",
    "            \n",
    "            L_mean_score.append(np.mean(L_score))\n",
    "            L_std_score.append(np.std(L_score))\n",
    "            \n",
    "        \n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "            \n",
    "        obs, _ = env.reset()\n",
    "        state = get_state(obs)\n",
    "        action = np.random.choice(np.arange(nA), p=get_probs(Q[state], epsilon, nA))\\\n",
    "                                                    if state in Q else env.action_space.sample()\n",
    "        \n",
    "        E = defaultdict(lambda: np.zeros(nA))\n",
    "\n",
    "        for _ in range(max_reward):\n",
    "            next_obs, reward, done, _, _ = env.step(action)\n",
    "            next_state = get_state(next_obs)\n",
    "\n",
    "            delta = reward - Q[state][action]\n",
    "            E[state][action] += 1 # accumulating traces\n",
    "            \n",
    "            if done:\n",
    "                for s in Q.keys():\n",
    "                    for a in range(nA):\n",
    "                        Q[s][a] += alpha * delta * E[s][a] \n",
    "                break\n",
    "            \n",
    "            \n",
    "            next_action = np.random.choice(np.arange(nA), p=get_probs(Q[next_state], epsilon, nA))\\\n",
    "                                                    if next_state in Q else env.action_space.sample()\n",
    "                                                    \n",
    "            delta += gamma * Q[next_state][next_action]\n",
    "            \n",
    "            for s in Q.keys():\n",
    "                for a in range(nA):\n",
    "                    Q[s][a] += alpha * delta * E[s][a]\n",
    "                    E[s][a] *= gamma * lmbda\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            \n",
    "    policy =  dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "    L_mean_score = np.array(L_mean_score)\n",
    "    L_std_score = np.array(L_std_score)\n",
    "    \n",
    "    return Q, policy, L_mean_score, L_std_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs 3-9 mins to run this cell\n",
    "sarsa_Q, sarsa_policy, sarsa_L_mean_score, sarsa_L_std_score = lambda_sarsa(env, num_episodes=20000, alpha=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_score = []\n",
    "for i in range(100):\n",
    "    L_score.append(one_play(env, sarsa_policy, graphic = False))\n",
    "print(f\"Mean score with Sarsa: {np.mean(L_score)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimated optimal state-value functions and policy of Sarsa($\\alpha$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_values(sarsa_Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To play TFB with `sarsa_policy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_play(env, sarsa_policy, graphic = True, max_step = 4000, FPS=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 : Comparison between MC and Sarsa($\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we would like to compare the performance of both algorithms applied in this problem. We study:\n",
    "- Their score evolution during training (by using `MC_L_mean_score`, `MC_L_std_score`, `sarsa_L_mean_score`, `sarsa_L_std_score` obtained above).\n",
    "- Their final score in function of $\\alpha$ the step-size parameter. To be more precise, at a fixed stepsize, we train the agents in $10000$ episodes, then test $100$ times the policies to return the mean and standard devision scores.\n",
    "- The influence of the environment configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score evolution during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i*1000 for i in range(len(MC_L_mean_score))], MC_L_mean_score, label=\"MC\")\n",
    "plt.fill_between([i*1000 for i in range(len(MC_L_mean_score))], MC_L_mean_score + MC_L_std_score*0.2,\n",
    "                 MC_L_mean_score - MC_L_std_score*0.2, alpha=0.2)\n",
    "\n",
    "# *0.2 at the std because we ran n=100 times the algorithm to get the mean score, so with 95%:\n",
    "# |mean_score - true_score| < std*2/sqrt(n) = 0.2 * std\n",
    "\n",
    "plt.plot([i*1000 for i in range(len(sarsa_L_mean_score))], sarsa_L_mean_score, label=r\"Sarsa($\\lambda$)\")\n",
    "plt.fill_between([i*1000 for i in range(len(sarsa_L_mean_score))], sarsa_L_mean_score + sarsa_L_std_score*0.2,\n",
    "                 sarsa_L_mean_score - sarsa_L_std_score*0.2, alpha=0.2)\n",
    "\n",
    "plt.xlabel(\"i-th episode\")\n",
    "plt.ylabel(\"Score for the\\n i-th episode\",rotation=0, labelpad=40)\n",
    "plt.title(\"Evolution of the player score during training\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final score in function of stepsize $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs 10-15 mins to run this cell\n",
    "\n",
    "L_step_sizes = np.logspace(-4, 0, num=10)\n",
    "\n",
    "num_episodes = 10000\n",
    "\n",
    "MC_L_score_stepsize = []\n",
    "sarsa_L_score_stepsize = []\n",
    "MC_L_std_score_stepsize = []\n",
    "sarsa_L_std_score_stepsize = []\n",
    "\n",
    "\n",
    "for i in range(len(L_step_sizes)):\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(f\"{i+1}-th iteration\")\n",
    "    \n",
    "    _, mc_policy, _, _ = mc_control(env, num_episodes, alpha=L_step_sizes[i], get_score_every_it=num_episodes)\n",
    "    \n",
    "    _, s_policy, _, _ = lambda_sarsa(env, num_episodes, alpha=L_step_sizes[i], get_score_every_it=num_episodes)\n",
    "    \n",
    "    mc_L_score = []\n",
    "    s_L_score = []\n",
    "    for _ in range(100):\n",
    "        mc_L_score.append(one_play(env, mc_policy, graphic = False))\n",
    "        s_L_score.append(one_play(env, s_policy, graphic = False))\n",
    "    \n",
    "    MC_L_score_stepsize.append(np.mean(mc_L_score))\n",
    "    MC_L_std_score_stepsize.append(np.std(mc_L_score))\n",
    "    \n",
    "    sarsa_L_score_stepsize.append(np.mean(s_L_score))\n",
    "    sarsa_L_std_score_stepsize.append(np.std(s_L_score))\n",
    "\n",
    "MC_L_score_stepsize = np.array(MC_L_score_stepsize)\n",
    "sarsa_L_score_stepsize = np.array(sarsa_L_score_stepsize)\n",
    "MC_L_std_score_stepsize = np.array(MC_L_std_score_stepsize)\n",
    "sarsa_L_std_score_stepsize = np.array(sarsa_L_std_score_stepsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(L_step_sizes, MC_L_score_stepsize, label=\"MC\")\n",
    "plt.fill_between(L_step_sizes, MC_L_score_stepsize + MC_L_std_score_stepsize*0.2,\n",
    "                 MC_L_score_stepsize - MC_L_std_score_stepsize*0.2, alpha=0.2)\n",
    "\n",
    "plt.plot(L_step_sizes, sarsa_L_score_stepsize, label=r\"Sarsa($\\lambda$)\")\n",
    "plt.fill_between(L_step_sizes, sarsa_L_score_stepsize + sarsa_L_std_score_stepsize*0.2,\n",
    "                 sarsa_L_score_stepsize - sarsa_L_std_score_stepsize*0.2, alpha=0.2)\n",
    "\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(f\"Mean score\\n after {num_episodes}\\nepisodes\",rotation=0, labelpad=40)\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Remark</u>: This graph shows us that $\\alpha \\approx 2 \\times 10^{-3}$ is the best value for MC, and $\\alpha \\approx 2 \\times 10^{-2}$ is the best for Sarsa($\\lambda$). This is why we have used these values for the training of both algorithms in the two first sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final score in function of $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs 14 mins to run this cell\n",
    "\n",
    "L_gamma = np.logspace(-2, 0, num=10, endpoint=True)\n",
    "\n",
    "num_episodes = 10000\n",
    "\n",
    "MC_L_score_gamma = []\n",
    "sarsa_L_score_gamma = []\n",
    "MC_L_std_score_gamma = []\n",
    "sarsa_L_std_score_gamma = []\n",
    "\n",
    "\n",
    "for i in range(len(L_gamma)):\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(f\"{i+1}-th iteration\")\n",
    "    \n",
    "    _, mc_policy, _, _ = mc_control(env, num_episodes, alpha=2e-3, gamma=L_gamma[i], \n",
    "                                    get_score_every_it=num_episodes)\n",
    "    \n",
    "    _, s_policy, _, _ = lambda_sarsa(env, num_episodes, alpha=2e-2, gamma=L_gamma[i], \n",
    "                                     get_score_every_it=num_episodes)\n",
    "    \n",
    "    mc_L_score = []\n",
    "    s_L_score = []\n",
    "    for _ in range(100):\n",
    "        mc_L_score.append(one_play(env, mc_policy, graphic = False))\n",
    "        s_L_score.append(one_play(env, s_policy, graphic = False))\n",
    "    \n",
    "    MC_L_score_gamma.append(np.mean(mc_L_score))\n",
    "    MC_L_std_score_gamma.append(np.std(mc_L_score))\n",
    "    \n",
    "    sarsa_L_score_gamma.append(np.mean(s_L_score))\n",
    "    sarsa_L_std_score_gamma.append(np.std(s_L_score))\n",
    "\n",
    "MC_L_score_gamma = np.array(MC_L_score_gamma)\n",
    "sarsa_L_score_gamma = np.array(sarsa_L_score_gamma)\n",
    "MC_L_std_score_gamma = np.array(MC_L_std_score_gamma)\n",
    "sarsa_L_std_score_gamma = np.array(sarsa_L_std_score_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(L_gamma, MC_L_score_gamma, label=\"MC\")\n",
    "plt.fill_between(L_gamma, MC_L_score_gamma + MC_L_std_score_gamma*0.2,\n",
    "                 MC_L_score_gamma - MC_L_std_score_gamma*0.2, alpha=0.2)\n",
    "\n",
    "plt.plot(L_gamma, sarsa_L_score_gamma, label=r\"Sarsa($\\lambda$)\")\n",
    "plt.fill_between(L_gamma, sarsa_L_score_gamma + sarsa_L_std_score_gamma*0.2,\n",
    "                 sarsa_L_score_gamma - sarsa_L_std_score_gamma*0.2, alpha=0.2)\n",
    "\n",
    "plt.xlabel(r\"$\\gamma$\")\n",
    "plt.ylabel(f\"Mean score after \\n{num_episodes}\\nepisodes\",rotation=0, labelpad=40)\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Remark</u>: This graph shows us that $\\gamma = 1.0$ is the best value for both MC and Sarsa($\\lambda$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final score in function of $\\lambda$ (only for Sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs 10 mins to run this cell\n",
    "\n",
    "L_lambda = np.logspace(-5, 0, num=10)\n",
    "\n",
    "num_episodes = 10000\n",
    "\n",
    "sarsa_L_score_lambda = []\n",
    "sarsa_L_std_score_lambda = []\n",
    "\n",
    "\n",
    "for i in range(len(L_lambda)):\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(f\"{i+1}-th iteration\")\n",
    "    \n",
    "    _, s_policy, _, _ = lambda_sarsa(env, num_episodes, alpha=2e-3, lmbda=L_lambda[i],\n",
    "                                    get_score_every_it=num_episodes)\n",
    "    \n",
    "    s_L_score = []\n",
    "    for _ in range(100):\n",
    "        s_L_score.append(one_play(env, s_policy, graphic = False))\n",
    "\n",
    "    sarsa_L_score_lambda.append(np.mean(s_L_score))\n",
    "    sarsa_L_std_score_lambda.append(np.std(s_L_score))\n",
    "\n",
    "sarsa_L_score_lambda = np.array(sarsa_L_score_lambda)\n",
    "sarsa_L_std_score_lambda = np.array(sarsa_L_std_score_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(L_lambda, sarsa_L_score_lambda, label=r\"Sarsa($\\lambda$)\")\n",
    "plt.fill_between(L_lambda, sarsa_L_score_lambda + sarsa_L_std_score_lambda*0.2,\n",
    "                 sarsa_L_score_lambda - sarsa_L_std_score_lambda*0.2, alpha=0.2)\n",
    "\n",
    "plt.xlabel(r\"$\\lambda$\")\n",
    "plt.ylabel(f\"Mean score after \\n{num_episodes}\\nepisodes\",rotation=0, labelpad=40)\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Remark</u>: The best value is $\\lambda = 1.0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still use the trained agents for 'TextFlappyBird-v0', if we change the y-axis orientation and rescale a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('TextFlappyBird-v0', height = 15, width = 20, pipe_gap = 4)\n",
    "max_step = 4000\n",
    "\n",
    "L_reward = []\n",
    "for i in range(100):\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    state_bis = (state[0],-1*state[1]+1)\n",
    "    sum_reward = 0\n",
    "\n",
    "    # iterate\n",
    "    for _ in range(max_step):\n",
    "        # Select next action\n",
    "        action = sarsa_policy[state_bis] if state_bis in sarsa_policy else env.action_space.sample()\n",
    "        # Appy action and return new observation of the environment\n",
    "        state, reward, done, _, info = env.step(action)\n",
    "        sum_reward += reward\n",
    "\n",
    "        state_bis = (state[0],-1*state[1]+1)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    L_reward.append(sum_reward)\n",
    "    \n",
    "print(f\"Mean total reward: {np.mean(L_reward)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try below different environment configuration by changing the window and the gap sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('TextFlappyBird-screen-v0', height = 15, width = 20, pipe_gap = 2)\n",
    "\n",
    "L_reward = []\n",
    "for i in range(100):\n",
    "    L_reward.append(one_play(env, sarsa_policy, graphic = False, return_type=\"reward\"))\n",
    "print(f\"Mean total reward: {np.mean(L_reward)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('TextFlappyBird-screen-v0', height = 15, width = 20, pipe_gap = 8)\n",
    "\n",
    "L_reward = []\n",
    "for i in range(100):\n",
    "    L_reward.append(one_play(env, sarsa_policy, graphic = False, return_type=\"reward\"))\n",
    "print(f\"Mean total reward: {np.mean(L_reward)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('TextFlappyBird-screen-v0', height = 20, width = 30, pipe_gap = 4)\n",
    "\n",
    "L_reward = []\n",
    "for i in range(100):\n",
    "    L_reward.append(one_play(env, sarsa_policy, graphic = False, return_type=\"reward\"))\n",
    "print(f\"Mean total reward: {np.mean(L_reward)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('TextFlappyBird-screen-v0', height = 10, width = 15, pipe_gap = 4)\n",
    "\n",
    "L_reward = []\n",
    "for i in range(100):\n",
    "    L_reward.append(one_play(env, sarsa_policy, graphic = False, return_type=\"reward\"))\n",
    "print(f\"Mean total reward: {np.mean(L_reward)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Agents work for bigger pipe gap, but not for smaller one.\n",
    "\n",
    "- For the height and width, as much as the state $(x,y)$ is defined in the policy, the agents can be used. That's why the agents perform well on smaller windows and not on bigger ones (many states $(x,y)$ in big windows have never been seen by the agents during training).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
